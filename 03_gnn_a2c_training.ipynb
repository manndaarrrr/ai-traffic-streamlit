{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9a0a24-ebc4-424e-81e6-17ca9424de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\manda\\OneDrive\\Documents\\AI Traffic - Jupyter\n",
      "Network exists?  True\n",
      "Routes exist?   True\n",
      "Config exists?  True\n",
      "SUMO_HOME: C:\\Program Files (x86)\\Eclipse\\Sumo\\\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import traci\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "\n",
    "NETWORK_FILE  = PROJECT_DIR / \"my_network.net.xml\"\n",
    "ROUTE_FILE    = PROJECT_DIR / \"my_routes.rou.xml\"\n",
    "CONFIG_FILE   = PROJECT_DIR / \"my_config.sumocfg\"\n",
    "\n",
    "print(\"Working directory:\", PROJECT_DIR)\n",
    "print(\"Network exists? \", NETWORK_FILE.exists())\n",
    "print(\"Routes exist?  \", ROUTE_FILE.exists())\n",
    "print(\"Config exists? \", CONFIG_FILE.exists())\n",
    "\n",
    "SUMO_HOME = os.environ.get(\"SUMO_HOME\")\n",
    "if SUMO_HOME is None:\n",
    "    raise EnvironmentError(\"SUMO_HOME is not set.\")\n",
    "\n",
    "print(\"SUMO_HOME:\", SUMO_HOME)\n",
    "\n",
    "\n",
    "def get_sumo_binary(gui: bool = False) -> str:\n",
    "    base_name = \"sumo-gui\" if gui else \"sumo\"\n",
    "\n",
    "    cmd = shutil.which(base_name)\n",
    "    if cmd is not None:\n",
    "        return cmd\n",
    "\n",
    "    bin_dir = Path(SUMO_HOME) / \"bin\"\n",
    "    if sys.platform.startswith(\"win\"):\n",
    "        candidate = bin_dir / f\"{base_name}.exe\"\n",
    "    else:\n",
    "        candidate = bin_dir / base_name\n",
    "\n",
    "    if not candidate.exists():\n",
    "        raise FileNotFoundError(f\"{base_name} not found at {candidate}\")\n",
    "\n",
    "    return str(candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2855fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d866538-df00-4721-ae3c-4a20a0d34d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TLS: 7\n",
      "('1234828897', '1757353212', '1757353214', '1783045940', '1783045985', '1843356909', '7671039164')\n",
      "\n",
      "TLS → lanes:\n",
      "1234828897 : ['107445428#1_0', '-173540663#0_0', '-47195458#0_0']\n",
      "1757353212 : ['1173473098#2_0', '107445426#1_0', '173540663#2_0']\n",
      "1757353214 : ['800859756#1_0', '800859756#1_1']\n",
      "1783045940 : ['-821520600#3_0', '315129223#3_0', '315129223#3_1', '821520600#2_0']\n",
      "1783045985 : ['-821520600#2_0', '821520600#1_0', '324489280#3_0']\n",
      "1843356909 : ['223051913#1_0', '223051913#1_1']\n",
      "7671039164 : ['164073716#1_0', '164073716#1_1', '164073716#1_2']\n"
     ]
    }
   ],
   "source": [
    "# --- Discover TLS IDs ---\n",
    "\n",
    "if traci.isLoaded():\n",
    "    traci.close()\n",
    "\n",
    "sumo_bin = get_sumo_binary(gui=False)\n",
    "cmd = [sumo_bin, \"-c\", str(CONFIG_FILE), \"--step-length\", \"1\"]\n",
    "traci.start(cmd)\n",
    "\n",
    "tls_ids = traci.trafficlight.getIDList()\n",
    "print(\"Total TLS:\", len(tls_ids))\n",
    "print(tls_ids)\n",
    "\n",
    "traci.close()\n",
    "\n",
    "# --- Build tls_lane_map ---\n",
    "\n",
    "if traci.isLoaded():\n",
    "    traci.close()\n",
    "\n",
    "traci.start([sumo_bin, \"-c\", str(CONFIG_FILE), \"--step-length\", \"1\"])\n",
    "\n",
    "tls_lane_map = {}\n",
    "for tls in tls_ids:\n",
    "    lanes = traci.trafficlight.getControlledLanes(tls)\n",
    "    lanes = list(dict.fromkeys(lanes))\n",
    "    tls_lane_map[tls] = lanes\n",
    "\n",
    "print(\"\\nTLS → lanes:\")\n",
    "for tls, lanes in tls_lane_map.items():\n",
    "    print(tls, \":\", lanes)\n",
    "\n",
    "traci.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52e9478-7125-426f-ac40-c639cc0ed208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State lengths per TLS: [7, 7, 5, 9, 7, 5, 7]\n",
      "feature_size: 9\n",
      "num_nodes: 7\n"
     ]
    }
   ],
   "source": [
    "def get_tls_state(tls_id: str, lane_map: dict) -> list:\n",
    "    lane_ids = lane_map[tls_id]\n",
    "    queue_lengths = []\n",
    "    waiting_times = []\n",
    "\n",
    "    for lane in lane_ids:\n",
    "        q = traci.lane.getLastStepHaltingNumber(lane)\n",
    "        w = traci.lane.getWaitingTime(lane)\n",
    "        queue_lengths.append(q)\n",
    "        waiting_times.append(w)\n",
    "\n",
    "    current_phase = traci.trafficlight.getPhase(tls_id)\n",
    "    return queue_lengths + waiting_times + [current_phase]\n",
    "\n",
    "\n",
    "# Compute feature_size\n",
    "if traci.isLoaded():\n",
    "    traci.close()\n",
    "\n",
    "traci.start([sumo_bin, \"-c\", str(CONFIG_FILE), \"--step-length\", \"1\"])\n",
    "\n",
    "for _ in range(5):\n",
    "    traci.simulationStep()\n",
    "\n",
    "lengths = []\n",
    "for tls in tls_ids:\n",
    "    s = get_tls_state(tls, tls_lane_map)\n",
    "    lengths.append(len(s))\n",
    "\n",
    "feature_size = max(lengths)\n",
    "num_nodes = len(tls_ids)\n",
    "\n",
    "print(\"State lengths per TLS:\", lengths)\n",
    "print(\"feature_size:\", feature_size)\n",
    "print(\"num_nodes:\", num_nodes)\n",
    "\n",
    "traci.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae4e835-9eea-4566-b120-3f56c8f89ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency list:\n",
      "1234828897 : []\n",
      "1757353212 : []\n",
      "1757353214 : []\n",
      "1783045940 : ['1783045985']\n",
      "1783045985 : ['1783045940']\n",
      "1843356909 : []\n",
      "7671039164 : []\n",
      "\n",
      "adj_matrix shape: (7, 7)\n",
      "adj_matrix[0]: [0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "if traci.isLoaded():\n",
    "    traci.close()\n",
    "\n",
    "traci.start([sumo_bin, \"-c\", str(CONFIG_FILE), \"--step-length\", \"1\"])\n",
    "\n",
    "tls_adj = {tls: set() for tls in tls_ids}\n",
    "\n",
    "for tls in tls_ids:\n",
    "    controlled_links = traci.trafficlight.getControlledLinks(tls)\n",
    "    for link_group in controlled_links:\n",
    "        for (incoming, outgoing, _) in link_group:\n",
    "            for other_tls in tls_ids:\n",
    "                if other_tls == tls:\n",
    "                    continue\n",
    "                if outgoing in tls_lane_map.get(other_tls, []):\n",
    "                    tls_adj[tls].add(other_tls)\n",
    "                    tls_adj[other_tls].add(tls)\n",
    "\n",
    "traci.close()\n",
    "\n",
    "edge_count = sum(len(neigh) for neigh in tls_adj.values())\n",
    "if edge_count == 0 and len(tls_ids) > 1:\n",
    "    print(\"No adjacency found; using simple chain.\")\n",
    "    ordered = list(tls_ids)\n",
    "    for i in range(len(ordered) - 1):\n",
    "        a, b = ordered[i], ordered[i+1]\n",
    "        tls_adj[a].add(b)\n",
    "        tls_adj[b].add(a)\n",
    "\n",
    "print(\"\\nAdjacency list:\")\n",
    "for tls, neigh in tls_adj.items():\n",
    "    print(tls, \":\", sorted(list(neigh)))\n",
    "\n",
    "# adjacency matrix\n",
    "tls_index = {tls_id: i for i, tls_id in enumerate(tls_ids)}\n",
    "adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "\n",
    "for tls, neigh in tls_adj.items():\n",
    "    i = tls_index[tls]\n",
    "    for nb in neigh:\n",
    "        j = tls_index[nb]\n",
    "        adj_matrix[i, j] = 1.0\n",
    "        adj_matrix[j, i] = 1.0\n",
    "\n",
    "print(\"\\nadj_matrix shape:\", adj_matrix.shape)\n",
    "print(\"adj_matrix[0]:\", adj_matrix[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237a4237-537d-4010-9ac6-d09c8b6ffa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_reward(tls_lane_map: dict) -> float:\n",
    "    total_wait = 0.0\n",
    "    for tls, lanes in tls_lane_map.items():\n",
    "        for lane in lanes:\n",
    "            total_wait += traci.lane.getWaitingTime(lane)\n",
    "    return -total_wait / 1000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47481e11-42f9-4c19-a8e2-9599ecaa4bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy logits shape: (1, 7, 2)\n",
      "Value shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "class GNNActorCritic(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim: int, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.state_embed = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.post_gnn   = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.policy_head = tf.keras.layers.Dense(num_actions)\n",
    "        self.value_head  = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, adj = inputs        # x: (B, N, F), adj: (B, N, N)\n",
    "\n",
    "        h = self.state_embed(x)      # (B, N, H)\n",
    "        h_neigh = tf.matmul(adj, h)  # (B, N, H)\n",
    "\n",
    "        h_cat = tf.concat([h, h_neigh], axis=-1)  # (B, N, 2H)\n",
    "        h_out = self.post_gnn(h_cat)              # (B, N, H)\n",
    "\n",
    "        policy_logits = self.policy_head(h_out)   # (B, N, A)\n",
    "\n",
    "        graph_embed = tf.reduce_mean(h_out, axis=1)  # (B, H)\n",
    "        value = self.value_head(graph_embed)         # (B, 1)\n",
    "\n",
    "        return policy_logits, value\n",
    "\n",
    "\n",
    "hidden_dim = 64\n",
    "num_actions = 2   # 0 = keep phase, 1 = switch\n",
    "\n",
    "gnn_model = GNNActorCritic(hidden_dim, num_actions)\n",
    "\n",
    "adj_batch_tf = tf.convert_to_tensor(adj_matrix[None, ...], dtype=tf.float32)\n",
    "\n",
    "# Build model with a dummy input\n",
    "dummy_states = tf.random.uniform((1, num_nodes, feature_size), dtype=tf.float32)\n",
    "logits_dummy, value_dummy = gnn_model((dummy_states, adj_batch_tf))\n",
    "\n",
    "print(\"Policy logits shape:\", logits_dummy.shape)  # (1, N, 2)\n",
    "print(\"Value shape:\", value_dummy.shape)           # (1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683169e1-009a-453c-870a-c9949010e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_actions_from_logits(policy_logits: tf.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Greedy action selection: argmax over actions per node.\n",
    "    policy_logits: (N, num_actions)\n",
    "    \"\"\"\n",
    "    if isinstance(policy_logits, tf.Tensor):\n",
    "        policy_logits = policy_logits.numpy()\n",
    "    return np.argmax(policy_logits, axis=-1)  # (N,)\n",
    "\n",
    "\n",
    "def apply_actions_to_sumo(actions: np.ndarray, tls_ids_list):\n",
    "    \"\"\"\n",
    "    actions[i] in {0,1} for TLS tls_ids_list[i]\n",
    "    0 = keep phase, 1 = switch to next phase\n",
    "    \"\"\"\n",
    "    for idx, tls in enumerate(tls_ids_list):\n",
    "        a = int(actions[idx])\n",
    "        if a == 0:\n",
    "            continue\n",
    "        elif a == 1:\n",
    "            curr_phase = traci.trafficlight.getPhase(tls)\n",
    "            logic = traci.trafficlight.getCompleteRedYellowGreenDefinition(tls)[0]\n",
    "            num_phases = len(logic.phases)\n",
    "            next_phase = (curr_phase + 1) % num_phases\n",
    "            traci.trafficlight.setPhase(tls, next_phase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4992195b-6440-411b-8a26-9186d788f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [0 0 0 0 0 0 0]\n",
      "One-step action test done.\n"
     ]
    }
   ],
   "source": [
    "if traci.isLoaded():\n",
    "    traci.close()\n",
    "\n",
    "traci.start([get_sumo_binary(False), \"-c\", str(CONFIG_FILE), \"--step-length\", \"1\"])\n",
    "traci.simulationStep()\n",
    "\n",
    "# build padded states\n",
    "all_states = []\n",
    "for tls in tls_ids:\n",
    "    s = get_tls_state(tls, tls_lane_map)\n",
    "    s_padded = s + [0] * (feature_size - len(s))\n",
    "    all_states.append(s_padded)\n",
    "\n",
    "states_np = np.array(all_states, dtype=np.float32)[None, ...]  # (1, N, F)\n",
    "states_tf = tf.convert_to_tensor(states_np, dtype=tf.float32)\n",
    "\n",
    "policy_logits_tf, value_tf = gnn_model((states_tf, adj_batch_tf))\n",
    "policy_logits = policy_logits_tf[0]  # (N, 2)\n",
    "\n",
    "actions = select_actions_from_logits(policy_logits)\n",
    "print(\"Actions:\", actions)\n",
    "\n",
    "apply_actions_to_sumo(actions, tls_ids)\n",
    "\n",
    "traci.close()\n",
    "print(\"One-step action test done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a2039f-2678-4db3-8e9d-7c6dc7a09b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manda\\AppData\\Local\\Temp\\ipykernel_13316\\1740942453.py:22: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
      "  logic = traci.trafficlight.getCompleteRedYellowGreenDefinition(tls)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300/3600, reward: -0.005\n",
      "Step 600/3600, reward: -0.001\n",
      "Step 900/3600, reward: -0.000\n",
      "Step 1200/3600, reward: -0.005\n",
      "Step 1500/3600, reward: -0.000\n",
      "Step 1800/3600, reward: -0.003\n",
      "Step 2100/3600, reward: -0.002\n",
      "Step 2400/3600, reward: -0.000\n",
      "Step 2700/3600, reward: -0.000\n",
      "Step 3000/3600, reward: -0.000\n",
      "Step 3300/3600, reward: -0.000\n",
      "Step 3600/3600, reward: -0.000\n",
      "Episode finished. Total return: -8.543999999999977\n",
      "Test episode return (untrained): -8.543999999999977\n"
     ]
    }
   ],
   "source": [
    "def run_one_episode(max_steps=3600) -> float:\n",
    "    \"\"\"\n",
    "    Run one full SUMO episode with the current (untrained) policy.\n",
    "    Returns total episode reward.\n",
    "    \"\"\"\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "\n",
    "    traci.start([get_sumo_binary(False), \"-c\", str(CONFIG_FILE), \"--step-length\", \"1\"])\n",
    "\n",
    "    episode_return = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # build states\n",
    "        all_states = []\n",
    "        for tls in tls_ids:\n",
    "            s = get_tls_state(tls, tls_lane_map)\n",
    "            s_padded = s + [0] * (feature_size - len(s))\n",
    "            all_states.append(s_padded)\n",
    "\n",
    "        states_np = np.array(all_states, dtype=np.float32)[None, ...]\n",
    "        states_tf = tf.convert_to_tensor(states_np, dtype=tf.float32)\n",
    "\n",
    "        policy_logits_tf, value_tf = gnn_model((states_tf, adj_batch_tf), training=False)\n",
    "        policy_logits = policy_logits_tf[0]\n",
    "\n",
    "        actions = select_actions_from_logits(policy_logits)\n",
    "        apply_actions_to_sumo(actions, tls_ids)\n",
    "\n",
    "        traci.simulationStep()\n",
    "\n",
    "        r = compute_global_reward(tls_lane_map)\n",
    "        episode_return += r\n",
    "\n",
    "        if (t + 1) % 300 == 0:\n",
    "            print(f\"Step {t+1}/{max_steps}, reward: {r:.3f}\")\n",
    "\n",
    "    traci.close()\n",
    "    print(\"Episode finished. Total return:\", episode_return)\n",
    "    return episode_return\n",
    "\n",
    "\n",
    "# Test with a shorter episode first, e.g. 600 steps\n",
    "test_return = run_one_episode(max_steps=3600)\n",
    "print(\"Test episode return (untrained):\", test_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b35845a-a7c1-466d-aba9-0c2851900ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A2C Hyperparameters ===\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)  # small LR for stability\n",
    "gamma = 0.99                                              # discount factor\n",
    "entropy_coef = 1e-3                                       # exploration bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c95a40e-284c-4259-adfc-fedf512ac328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode(max_steps=900) -> float:\n",
    "    \"\"\"\n",
    "    Run one training episode using online A2C updates.\n",
    "    Returns total episode return (sum of rewards).\n",
    "    \"\"\"\n",
    "\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "\n",
    "    traci.start([get_sumo_binary(False), \"-c\", str(CONFIG_FILE), \"--step-length\", \"1\"])\n",
    "\n",
    "    episode_return = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # ---- 1) Build current state batch: (1, N, F) ----\n",
    "        all_states = []\n",
    "        for tls in tls_ids:\n",
    "            s = get_tls_state(tls, tls_lane_map)\n",
    "            s_padded = s + [0] * (feature_size - len(s))\n",
    "            all_states.append(s_padded)\n",
    "\n",
    "        states_np = np.array(all_states, dtype=np.float32)[None, ...]\n",
    "        states_tf = tf.convert_to_tensor(states_np, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # ---- 2) Forward pass ----\n",
    "            policy_logits_tf, value_tf = gnn_model((states_tf, adj_batch_tf), training=True)\n",
    "            policy_logits = policy_logits_tf[0]     # (N, num_actions)\n",
    "\n",
    "            # Probabilities\n",
    "            action_probs = tf.nn.softmax(policy_logits, axis=-1)     # (N, num_actions)\n",
    "\n",
    "            # ---- 3) Sample actions (stochastic) ----\n",
    "            actions_tf = tf.random.categorical(tf.math.log(action_probs), num_samples=1)\n",
    "            actions_tf = tf.squeeze(actions_tf, axis=-1)             # (N,)\n",
    "            actions_np = actions_tf.numpy()\n",
    "\n",
    "            # Apply actions in SUMO\n",
    "            apply_actions_to_sumo(actions_np, tls_ids)\n",
    "\n",
    "            # ---- 4) Step SUMO ----\n",
    "            traci.simulationStep()\n",
    "\n",
    "            # ---- 5) Reward ----\n",
    "            r = compute_global_reward(tls_lane_map)   # negative, scaled\n",
    "            episode_return += r\n",
    "\n",
    "            # ---- 6) Next state for TD target ----\n",
    "            next_states_list = []\n",
    "            for tls in tls_ids:\n",
    "                s_next = get_tls_state(tls, tls_lane_map)\n",
    "                s_next_padded = s_next + [0] * (feature_size - len(s_next))\n",
    "                next_states_list.append(s_next_padded)\n",
    "\n",
    "            next_states_np = np.array(next_states_list, dtype=np.float32)[None, ...]\n",
    "            next_states_tf = tf.convert_to_tensor(next_states_np, dtype=tf.float32)\n",
    "\n",
    "            _, next_value_tf = gnn_model((next_states_tf, adj_batch_tf), training=False)\n",
    "\n",
    "            # v and v_next are scalars (shape (1,1))\n",
    "            v = tf.squeeze(value_tf)\n",
    "            v_next = tf.squeeze(next_value_tf)\n",
    "\n",
    "            # ---- 7) TD target & advantage ----\n",
    "            v_next_detached = tf.stop_gradient(v_next)\n",
    "            td_target = r + gamma * v_next_detached\n",
    "            advantage = td_target - v\n",
    "\n",
    "            # ---- 8) Losses ----\n",
    "            log_probs = tf.nn.log_softmax(policy_logits, axis=-1)    # (N, num_actions)\n",
    "\n",
    "            idx = tf.stack([\n",
    "                tf.range(tf.shape(actions_tf)[0], dtype=tf.int32),\n",
    "                tf.cast(actions_tf, tf.int32)\n",
    "            ], axis=1)                                              # (N, 2)\n",
    "\n",
    "            chosen_log_probs = tf.gather_nd(log_probs, idx)         # (N,)\n",
    "\n",
    "            actor_loss = -tf.reduce_mean(chosen_log_probs * advantage)\n",
    "            critic_loss = tf.reduce_mean(tf.square(td_target - v))\n",
    "\n",
    "            entropy = -tf.reduce_mean(action_probs * tf.math.log(action_probs + 1e-8))\n",
    "\n",
    "            loss = actor_loss + 0.5 * critic_loss - entropy_coef * entropy\n",
    "\n",
    "        # ---- 9) Backprop ----\n",
    "        grads = tape.gradient(loss, gnn_model.trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 0.5)\n",
    "        optimizer.apply_gradients(zip(grads, gnn_model.trainable_variables))\n",
    "        if (t + 1) % 300 == 0:\n",
    "            print(\n",
    "                f\"[Train] Step {t+1}/{max_steps}, \"\n",
    "                f\"reward: {r:.4f}, \"\n",
    "                f\"loss: {loss.numpy():.4f}\"\n",
    "            )\n",
    "\n",
    "    traci.close()\n",
    "    print(\"Training episode finished. Total return:\", episode_return)\n",
    "    return episode_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82eacfc7-4ce0-4775-890b-0e01a7240b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING EPISODE 1/50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manda\\AppData\\Local\\Temp\\ipykernel_13316\\1740942453.py:22: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
      "  logic = traci.trafficlight.getCompleteRedYellowGreenDefinition(tls)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step 300/900, reward: -0.0060, loss: 0.4599\n",
      "[Train] Step 600/900, reward: -0.0120, loss: 1.7620\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.2225\n",
      "Training episode finished. Total return: -2.2239999999999815\n",
      "Episode 1 return: -2.2240\n",
      "\n",
      "=== TRAINING EPISODE 2/50 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 0.5023\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 2.6128\n",
      "[Train] Step 900/900, reward: -0.0050, loss: 0.6078\n",
      "Training episode finished. Total return: -2.2719999999999856\n",
      "Episode 2 return: -2.2720\n",
      "\n",
      "=== TRAINING EPISODE 3/50 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 0.0402\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 2.5701\n",
      "[Train] Step 900/900, reward: -0.0050, loss: -0.1584\n",
      "Training episode finished. Total return: -6.783999999999999\n",
      "Episode 3 return: -6.7840\n",
      "\n",
      "=== TRAINING EPISODE 4/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 1.8683\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.4698\n",
      "[Train] Step 900/900, reward: -0.0000, loss: -0.1252\n",
      "Training episode finished. Total return: -1.9989999999999846\n",
      "Episode 4 return: -1.9990\n",
      "\n",
      "=== TRAINING EPISODE 5/50 ===\n",
      "[Train] Step 300/900, reward: -0.0040, loss: 0.3822\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 5.5795\n",
      "[Train] Step 900/900, reward: -0.0000, loss: -0.2469\n",
      "Training episode finished. Total return: -2.17799999999998\n",
      "Episode 5 return: -2.1780\n",
      "\n",
      "=== TRAINING EPISODE 6/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.5305\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.7498\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.2913\n",
      "Training episode finished. Total return: -19.37999999999999\n",
      "Episode 6 return: -19.3800\n",
      "\n",
      "=== TRAINING EPISODE 7/50 ===\n",
      "[Train] Step 300/900, reward: -0.0060, loss: 3.1191\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 17.7566\n",
      "[Train] Step 900/900, reward: -0.0130, loss: -0.2091\n",
      "Training episode finished. Total return: -1.7449999999999897\n",
      "Episode 7 return: -1.7450\n",
      "\n",
      "=== TRAINING EPISODE 8/50 ===\n",
      "[Train] Step 300/900, reward: -0.0020, loss: 2.0614\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 6.2973\n",
      "[Train] Step 900/900, reward: -0.0000, loss: -0.1490\n",
      "Training episode finished. Total return: -8.379000000000003\n",
      "Episode 8 return: -8.3790\n",
      "\n",
      "=== TRAINING EPISODE 9/50 ===\n",
      "[Train] Step 300/900, reward: -0.0410, loss: 0.4916\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 8.4620\n",
      "[Train] Step 900/900, reward: -0.0020, loss: 1.1651\n",
      "Training episode finished. Total return: -4.049999999999984\n",
      "Episode 9 return: -4.0500\n",
      "\n",
      "=== TRAINING EPISODE 10/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: -0.2487\n",
      "[Train] Step 600/900, reward: -0.0010, loss: 3.1930\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 1.1244\n",
      "Training episode finished. Total return: -3.4499999999999713\n",
      "Episode 10 return: -3.4500\n",
      "\n",
      "=== TRAINING EPISODE 11/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 2.1892\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 4.4289\n",
      "[Train] Step 900/900, reward: -0.1670, loss: -0.0187\n",
      "Training episode finished. Total return: -10.214999999999977\n",
      "Episode 11 return: -10.2150\n",
      "\n",
      "=== TRAINING EPISODE 12/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.4715\n",
      "[Train] Step 600/900, reward: -0.0120, loss: 1.5156\n",
      "[Train] Step 900/900, reward: -0.0030, loss: 2.8088\n",
      "Training episode finished. Total return: -12.055000000000005\n",
      "Episode 12 return: -12.0550\n",
      "\n",
      "=== TRAINING EPISODE 13/50 ===\n",
      "[Train] Step 300/900, reward: -0.0020, loss: 5.3992\n",
      "[Train] Step 600/900, reward: -0.0000, loss: -0.0472\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 7.2130\n",
      "Training episode finished. Total return: -3.533999999999974\n",
      "Episode 13 return: -3.5340\n",
      "\n",
      "=== TRAINING EPISODE 14/50 ===\n",
      "[Train] Step 300/900, reward: -0.0040, loss: 1.5697\n",
      "[Train] Step 600/900, reward: -0.0130, loss: 0.4313\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 11.0773\n",
      "Training episode finished. Total return: -4.444999999999984\n",
      "Episode 14 return: -4.4450\n",
      "\n",
      "=== TRAINING EPISODE 15/50 ===\n",
      "[Train] Step 300/900, reward: -0.0050, loss: -0.0036\n",
      "[Train] Step 600/900, reward: -0.0000, loss: -0.0758\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.2694\n",
      "Training episode finished. Total return: -2.9189999999999734\n",
      "Episode 15 return: -2.9190\n",
      "\n",
      "=== TRAINING EPISODE 16/50 ===\n",
      "[Train] Step 300/900, reward: -0.0040, loss: 0.8100\n",
      "[Train] Step 600/900, reward: -0.0050, loss: 0.0873\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 3.9803\n",
      "Training episode finished. Total return: -1.6889999999999918\n",
      "Episode 16 return: -1.6890\n",
      "\n",
      "=== TRAINING EPISODE 17/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.7653\n",
      "[Train] Step 600/900, reward: -0.0020, loss: 0.2193\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 4.4374\n",
      "Training episode finished. Total return: -10.386999999999995\n",
      "Episode 17 return: -10.3870\n",
      "\n",
      "=== TRAINING EPISODE 18/50 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 2.7474\n",
      "[Train] Step 600/900, reward: -0.0120, loss: 0.5330\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 3.9007\n",
      "Training episode finished. Total return: -1.9899999999999887\n",
      "Episode 18 return: -1.9900\n",
      "\n",
      "=== TRAINING EPISODE 19/50 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 1.7260\n",
      "[Train] Step 600/900, reward: -0.0230, loss: -0.0249\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.1852\n",
      "Training episode finished. Total return: -12.535999999999989\n",
      "Episode 19 return: -12.5360\n",
      "\n",
      "=== TRAINING EPISODE 20/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.3500\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 6.1361\n",
      "[Train] Step 900/900, reward: -0.0060, loss: 5.7793\n",
      "Training episode finished. Total return: -11.480999999999993\n",
      "Episode 20 return: -11.4810\n",
      "\n",
      "=== TRAINING EPISODE 21/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: -0.1398\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.4837\n",
      "[Train] Step 900/900, reward: -0.0120, loss: 7.9769\n",
      "Training episode finished. Total return: -3.1259999999999843\n",
      "Episode 21 return: -3.1260\n",
      "\n",
      "=== TRAINING EPISODE 22/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 1.6860\n",
      "[Train] Step 600/900, reward: -0.0140, loss: 9.4358\n",
      "[Train] Step 900/900, reward: -0.0000, loss: -0.1556\n",
      "Training episode finished. Total return: -2.260999999999979\n",
      "Episode 22 return: -2.2610\n",
      "\n",
      "=== TRAINING EPISODE 23/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 1.5252\n",
      "[Train] Step 600/900, reward: -0.0120, loss: 1.5324\n",
      "[Train] Step 900/900, reward: -0.1080, loss: 0.3079\n",
      "Training episode finished. Total return: -16.064999999999994\n",
      "Episode 23 return: -16.0650\n",
      "\n",
      "=== TRAINING EPISODE 24/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.4110\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 5.1195\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 27.4330\n",
      "Training episode finished. Total return: -13.190999999999995\n",
      "Episode 24 return: -13.1910\n",
      "\n",
      "=== TRAINING EPISODE 25/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.2572\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 3.7836\n",
      "[Train] Step 900/900, reward: -0.0030, loss: 0.5681\n",
      "Training episode finished. Total return: -4.288999999999976\n",
      "Episode 25 return: -4.2890\n",
      "\n",
      "=== TRAINING EPISODE 26/50 ===\n",
      "[Train] Step 300/900, reward: -0.0020, loss: 5.6483\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 5.3225\n",
      "[Train] Step 900/900, reward: -0.0070, loss: 0.4733\n",
      "Training episode finished. Total return: -1.8279999999999872\n",
      "Episode 26 return: -1.8280\n",
      "\n",
      "=== TRAINING EPISODE 27/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.1738\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 1.9783\n",
      "[Train] Step 900/900, reward: -0.0000, loss: -0.1898\n",
      "Training episode finished. Total return: -3.1479999999999766\n",
      "Episode 27 return: -3.1480\n",
      "\n",
      "=== TRAINING EPISODE 28/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.6309\n",
      "[Train] Step 600/900, reward: -0.0100, loss: 0.0195\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.4050\n",
      "Training episode finished. Total return: -1.5259999999999947\n",
      "Episode 28 return: -1.5260\n",
      "\n",
      "=== TRAINING EPISODE 29/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 2.8357\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.3664\n",
      "[Train] Step 900/900, reward: -0.0800, loss: 7.7390\n",
      "Training episode finished. Total return: -13.930000000000009\n",
      "Episode 29 return: -13.9300\n",
      "\n",
      "=== TRAINING EPISODE 30/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 4.7336\n",
      "[Train] Step 600/900, reward: -0.0010, loss: 2.9890\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 19.4314\n",
      "Training episode finished. Total return: -2.564999999999986\n",
      "Episode 30 return: -2.5650\n",
      "\n",
      "=== TRAINING EPISODE 31/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 4.9834\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.2706\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 9.2682\n",
      "Training episode finished. Total return: -3.6849999999999743\n",
      "Episode 31 return: -3.6850\n",
      "\n",
      "=== TRAINING EPISODE 32/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 1.1853\n",
      "[Train] Step 600/900, reward: -0.0020, loss: 5.0466\n",
      "[Train] Step 900/900, reward: -0.0190, loss: 10.8189\n",
      "Training episode finished. Total return: -5.795999999999998\n",
      "Episode 32 return: -5.7960\n",
      "\n",
      "=== TRAINING EPISODE 33/50 ===\n",
      "[Train] Step 300/900, reward: -0.0050, loss: 19.6994\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.8293\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.0859\n",
      "Training episode finished. Total return: -1.6329999999999956\n",
      "Episode 33 return: -1.6330\n",
      "\n",
      "=== TRAINING EPISODE 34/50 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 1.4337\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 3.2622\n",
      "[Train] Step 900/900, reward: -0.0040, loss: -0.0201\n",
      "Training episode finished. Total return: -3.3779999999999855\n",
      "Episode 34 return: -3.3780\n",
      "\n",
      "=== TRAINING EPISODE 35/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.0857\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.0282\n",
      "[Train] Step 900/900, reward: -0.1290, loss: 0.0197\n",
      "Training episode finished. Total return: -11.685999999999986\n",
      "Episode 35 return: -11.6860\n",
      "\n",
      "=== TRAINING EPISODE 36/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 5.4514\n",
      "[Train] Step 600/900, reward: -0.0290, loss: 3.9138\n",
      "[Train] Step 900/900, reward: -0.0010, loss: 0.2257\n",
      "Training episode finished. Total return: -18.64100000000001\n",
      "Episode 36 return: -18.6410\n",
      "\n",
      "=== TRAINING EPISODE 37/50 ===\n",
      "[Train] Step 300/900, reward: -0.0080, loss: 10.2909\n",
      "[Train] Step 600/900, reward: -0.0140, loss: 5.0968\n",
      "[Train] Step 900/900, reward: -0.0010, loss: -0.0768\n",
      "Training episode finished. Total return: -10.424999999999994\n",
      "Episode 37 return: -10.4250\n",
      "\n",
      "=== TRAINING EPISODE 38/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: -0.0238\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.1853\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 13.0278\n",
      "Training episode finished. Total return: -2.837999999999993\n",
      "Episode 38 return: -2.8380\n",
      "\n",
      "=== TRAINING EPISODE 39/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: -0.0026\n",
      "[Train] Step 600/900, reward: -0.0110, loss: 5.9992\n",
      "[Train] Step 900/900, reward: -0.0260, loss: 12.7556\n",
      "Training episode finished. Total return: -3.336999999999989\n",
      "Episode 39 return: -3.3370\n",
      "\n",
      "=== TRAINING EPISODE 40/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 2.8232\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 11.3448\n",
      "[Train] Step 900/900, reward: -0.0030, loss: 1.2431\n",
      "Training episode finished. Total return: -5.001999999999989\n",
      "Episode 40 return: -5.0020\n",
      "\n",
      "=== TRAINING EPISODE 41/50 ===\n",
      "[Train] Step 300/900, reward: -0.0080, loss: 0.2608\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 5.2739\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 15.3779\n",
      "Training episode finished. Total return: -10.952999999999994\n",
      "Episode 41 return: -10.9530\n",
      "\n",
      "=== TRAINING EPISODE 42/50 ===\n",
      "[Train] Step 300/900, reward: -0.0520, loss: 0.0439\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 2.5275\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 1.2035\n",
      "Training episode finished. Total return: -8.947999999999988\n",
      "Episode 42 return: -8.9480\n",
      "\n",
      "=== TRAINING EPISODE 43/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: -0.0013\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 2.3247\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 3.7625\n",
      "Training episode finished. Total return: -1.397999999999995\n",
      "Episode 43 return: -1.3980\n",
      "\n",
      "=== TRAINING EPISODE 44/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 11.2870\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 9.1490\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 52.7365\n",
      "Training episode finished. Total return: -13.149999999999997\n",
      "Episode 44 return: -13.1500\n",
      "\n",
      "=== TRAINING EPISODE 45/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.2480\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.3757\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 6.2156\n",
      "Training episode finished. Total return: -16.94700000000003\n",
      "Episode 45 return: -16.9470\n",
      "\n",
      "=== TRAINING EPISODE 46/50 ===\n",
      "[Train] Step 300/900, reward: -0.0050, loss: 0.0573\n",
      "[Train] Step 600/900, reward: -0.0010, loss: 1.0993\n",
      "[Train] Step 900/900, reward: -0.0770, loss: 9.3388\n",
      "Training episode finished. Total return: -9.428999999999997\n",
      "Episode 46 return: -9.4290\n",
      "\n",
      "=== TRAINING EPISODE 47/50 ===\n",
      "[Train] Step 300/900, reward: -0.2100, loss: 1.0083\n",
      "[Train] Step 600/900, reward: -0.0130, loss: 18.9698\n",
      "[Train] Step 900/900, reward: -0.0060, loss: 0.0346\n",
      "Training episode finished. Total return: -37.76299999999996\n",
      "Episode 47 return: -37.7630\n",
      "\n",
      "=== TRAINING EPISODE 48/50 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 1.9792\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 1.5639\n",
      "[Train] Step 900/900, reward: -0.0210, loss: 14.8804\n",
      "Training episode finished. Total return: -5.368999999999975\n",
      "Episode 48 return: -5.3690\n",
      "\n",
      "=== TRAINING EPISODE 49/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 7.7861\n",
      "[Train] Step 600/900, reward: -0.0010, loss: 1.7909\n",
      "[Train] Step 900/900, reward: -0.0700, loss: 0.4762\n",
      "Training episode finished. Total return: -3.6869999999999976\n",
      "Episode 49 return: -3.6870\n",
      "\n",
      "=== TRAINING EPISODE 50/50 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 0.2060\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 11.0323\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 2.5995\n",
      "Training episode finished. Total return: -2.2439999999999873\n",
      "Episode 50 return: -2.2440\n",
      "\n",
      "All training returns: [-2.2239999999999815, -2.2719999999999856, -6.783999999999999, -1.9989999999999846, -2.17799999999998, -19.37999999999999, -1.7449999999999897, -8.379000000000003, -4.049999999999984, -3.4499999999999713, -10.214999999999977, -12.055000000000005, -3.533999999999974, -4.444999999999984, -2.9189999999999734, -1.6889999999999918, -10.386999999999995, -1.9899999999999887, -12.535999999999989, -11.480999999999993, -3.1259999999999843, -2.260999999999979, -16.064999999999994, -13.190999999999995, -4.288999999999976, -1.8279999999999872, -3.1479999999999766, -1.5259999999999947, -13.930000000000009, -2.564999999999986, -3.6849999999999743, -5.795999999999998, -1.6329999999999956, -3.3779999999999855, -11.685999999999986, -18.64100000000001, -10.424999999999994, -2.837999999999993, -3.336999999999989, -5.001999999999989, -10.952999999999994, -8.947999999999988, -1.397999999999995, -13.149999999999997, -16.94700000000003, -9.428999999999997, -37.76299999999996, -5.368999999999975, -3.6869999999999976, -2.2439999999999873]\n"
     ]
    }
   ],
   "source": [
    "train_returns = []\n",
    "num_train_episodes = 50  # you can increase later\n",
    "\n",
    "for ep in range(num_train_episodes):\n",
    "    print(f\"\\n=== TRAINING EPISODE {ep+1}/{num_train_episodes} ===\")\n",
    "    ep_ret = train_one_episode(max_steps=900)\n",
    "    train_returns.append(ep_ret)\n",
    "    print(f\"Episode {ep+1} return: {ep_ret:.4f}\")\n",
    "\n",
    "print(\"\\nAll training returns:\", train_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e9f29d4-a808-499a-a7a7-6a072c3b8f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manda\\AppData\\Local\\Temp\\ipykernel_13316\\1740942453.py:22: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
      "  logic = traci.trafficlight.getCompleteRedYellowGreenDefinition(tls)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300/3600, reward: -0.000\n",
      "Step 600/3600, reward: -0.015\n",
      "Step 900/3600, reward: -0.006\n",
      "Step 1200/3600, reward: -0.007\n",
      "Step 1500/3600, reward: -0.000\n",
      "Step 1800/3600, reward: -0.001\n",
      "Step 2100/3600, reward: -0.000\n",
      "Step 2400/3600, reward: -0.005\n",
      "Step 2700/3600, reward: -0.001\n",
      "Step 3000/3600, reward: -0.000\n",
      "Step 3300/3600, reward: -0.001\n",
      "Step 3600/3600, reward: -0.005\n",
      "Episode finished. Total return: -24.461000000000077\n",
      "Trained policy episode return: -24.461000000000077\n"
     ]
    }
   ],
   "source": [
    "# make sure run_one_episode uses select_actions_from_logits (argmax)\n",
    "test_return_trained = run_one_episode(max_steps=3600)\n",
    "print(\"Trained policy episode return:\", test_return_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20e497e4-7136-48e2-9327-89914c61d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING EPISODE 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manda\\AppData\\Local\\Temp\\ipykernel_13316\\1740942453.py:22: UserWarning: Call to deprecated function getAllProgramLogics, use getCompleteRedYellowGreenDefinition instead.\n",
      "  logic = traci.trafficlight.getCompleteRedYellowGreenDefinition(tls)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step 300/900, reward: -0.0000, loss: 1.8929\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 2.6095\n",
      "[Train] Step 900/900, reward: -0.0810, loss: 0.5834\n",
      "Training episode finished. Total return: -3.815999999999996\n",
      "Episode 1 return: -3.8160\n",
      "--> New best model saved with return -3.8160\n",
      "\n",
      "=== TRAINING EPISODE 2/10 ===\n",
      "[Train] Step 300/900, reward: -0.1200, loss: 7.2763\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.5784\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.0070\n",
      "Training episode finished. Total return: -20.997000000000067\n",
      "Episode 2 return: -20.9970\n",
      "\n",
      "=== TRAINING EPISODE 3/10 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 6.3848\n",
      "[Train] Step 600/900, reward: -0.0010, loss: 12.7989\n",
      "[Train] Step 900/900, reward: -0.0060, loss: 5.4263\n",
      "Training episode finished. Total return: -5.343999999999981\n",
      "Episode 3 return: -5.3440\n",
      "\n",
      "=== TRAINING EPISODE 4/10 ===\n",
      "[Train] Step 300/900, reward: -0.0020, loss: 2.2984\n",
      "[Train] Step 600/900, reward: -0.0170, loss: 4.0526\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.1027\n",
      "Training episode finished. Total return: -6.503000000000008\n",
      "Episode 4 return: -6.5030\n",
      "\n",
      "=== TRAINING EPISODE 5/10 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 2.9350\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.5137\n",
      "[Train] Step 900/900, reward: -0.0050, loss: 4.4756\n",
      "Training episode finished. Total return: -6.09400000000001\n",
      "Episode 5 return: -6.0940\n",
      "\n",
      "=== TRAINING EPISODE 6/10 ===\n",
      "[Train] Step 300/900, reward: -0.0040, loss: 0.2869\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.7808\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 1.3299\n",
      "Training episode finished. Total return: -7.892999999999996\n",
      "Episode 6 return: -7.8930\n",
      "\n",
      "=== TRAINING EPISODE 7/10 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 2.5280\n",
      "[Train] Step 600/900, reward: -0.0180, loss: 5.9786\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.0114\n",
      "Training episode finished. Total return: -2.784999999999993\n",
      "Episode 7 return: -2.7850\n",
      "--> New best model saved with return -2.7850\n",
      "\n",
      "=== TRAINING EPISODE 8/10 ===\n",
      "[Train] Step 300/900, reward: -0.0000, loss: 2.8145\n",
      "[Train] Step 600/900, reward: -0.0190, loss: 14.8420\n",
      "[Train] Step 900/900, reward: -0.1590, loss: 0.9020\n",
      "Training episode finished. Total return: -25.495999999999988\n",
      "Episode 8 return: -25.4960\n",
      "\n",
      "=== TRAINING EPISODE 9/10 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 0.3634\n",
      "[Train] Step 600/900, reward: -0.0110, loss: 0.0673\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.1042\n",
      "Training episode finished. Total return: -1.4819999999999947\n",
      "Episode 9 return: -1.4820\n",
      "--> New best model saved with return -1.4820\n",
      "\n",
      "=== TRAINING EPISODE 10/10 ===\n",
      "[Train] Step 300/900, reward: -0.0010, loss: 0.8731\n",
      "[Train] Step 600/900, reward: -0.0000, loss: 0.0744\n",
      "[Train] Step 900/900, reward: -0.0000, loss: 0.4470\n",
      "Training episode finished. Total return: -7.141000000000013\n",
      "Episode 10 return: -7.1410\n"
     ]
    }
   ],
   "source": [
    "best_return = -1e9  # very small\n",
    "\n",
    "for ep in range(10):\n",
    "    print(f\"\\n=== TRAINING EPISODE {ep+1}/10 ===\")\n",
    "    ep_ret = train_one_episode(max_steps=900)\n",
    "    train_returns.append(ep_ret)\n",
    "    print(f\"Episode {ep+1} return: {ep_ret:.4f}\")\n",
    "\n",
    "    if ep_ret > best_return:\n",
    "        best_return = ep_ret\n",
    "        gnn_model.save_weights(\"gnn_a2c_best.weights.h5\")\n",
    "        print(f\"--> New best model saved with return {best_return:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a09d4a-c764-4d5a-ae97-5a61247c3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ FINAL PPO TRAINING CODE ================================\n",
    "# WORKS WITH YOUR GNN MODEL, TLS SETUP, adj_batch_tf, tls_ids, tls_lane_map, CONFIG_FILE\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import traci\n",
    "\n",
    "\n",
    "# ---------------- PPO Hyperparameters ----------------\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "LR = 1e-4\n",
    "EPOCHS = 4\n",
    "ENTROPY_COEFF = 0.01\n",
    "VALUE_COEFF = 0.5\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# utility: convert multi-TLS actions → 1 flattened PPO action\n",
    "# ===================================================================\n",
    "def flatten_action(action_vec, n_actions):\n",
    "    \"\"\"Convert TLS action vector like [0,1,0,1] → a single int.\"\"\"\n",
    "    code = 0\n",
    "    base = 1\n",
    "    for a in reversed(action_vec):\n",
    "        code += a * base\n",
    "        base *= n_actions\n",
    "    return code\n",
    "\n",
    "\n",
    "def unflatten_action(code, num_tls, n_actions):\n",
    "    \"\"\"Convert flattened int → TLS vector like [0,1,0,1].\"\"\"\n",
    "    out = []\n",
    "    for _ in range(num_tls):\n",
    "        out.append(code % n_actions)\n",
    "        code //= n_actions\n",
    "    return out[::-1]\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# reward function (stable)\n",
    "# ===================================================================\n",
    "def compute_reward(tls_lane_map):\n",
    "    total_wait = sum(traci.lane.getWaitingTime(l) \n",
    "                     for tls in tls_lane_map for l in tls_lane_map[tls])\n",
    "    return -np.tanh(total_wait / 1000)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# compute GAE advantages + returns\n",
    "# ===================================================================\n",
    "def compute_advantages(rewards, values):\n",
    "    advantages = []\n",
    "    gae = 0.0\n",
    "    values = values + [0.0]  # bootstrap\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + GAMMA * values[t+1] - values[t]\n",
    "        gae = delta + GAMMA * LAMBDA * gae\n",
    "        advantages.append(gae)\n",
    "\n",
    "    advantages.reverse()\n",
    "    returns = [advantages[i] + values[i] for i in range(len(rewards))]\n",
    "    return np.array(advantages, dtype=np.float32), np.array(returns, dtype=np.float32)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PPO update function\n",
    "# ===================================================================\n",
    "def ppo_update(states, actions, old_log_probs, returns, advantages):\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "    old_log_probs = tf.convert_to_tensor(old_log_probs, dtype=tf.float32)\n",
    "    returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "\n",
    "    dataset_size = states.shape[0]\n",
    "\n",
    "    for _ in range(EPOCHS):\n",
    "        with tf.GradientTape() as tape:\n",
    "            adj_batch = tf.repeat(adj_batch_tf, repeats=dataset_size, axis=0)\n",
    "\n",
    "            logits, values = gnn_model((states, adj_batch), training=True)\n",
    "            values = tf.squeeze(values, axis=1)\n",
    "\n",
    "            # flatten logits → categorical distribution over joint action space\n",
    "            flat_logits = tf.reshape(logits, (dataset_size, -1))\n",
    "            probs = tf.nn.softmax(flat_logits)\n",
    "\n",
    "            # pick chosen action log-prob\n",
    "            indices = tf.stack([tf.range(dataset_size), actions], axis=1)\n",
    "            new_log_probs = tf.math.log(tf.gather_nd(probs, indices) + 1e-10)\n",
    "\n",
    "            ratio = tf.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            unclipped = ratio * advantages\n",
    "            clipped = tf.clip_by_value(ratio, 1-CLIP_EPS, 1+CLIP_EPS) * advantages\n",
    "            policy_loss = -tf.reduce_mean(tf.minimum(unclipped, clipped))\n",
    "\n",
    "            value_loss = VALUE_COEFF * tf.reduce_mean((returns - values)**2)\n",
    "\n",
    "            entropy = -tf.reduce_mean(tf.reduce_sum(probs * tf.math.log(probs + 1e-10), axis=1))\n",
    "\n",
    "            loss = policy_loss + value_loss - ENTROPY_COEFF * entropy\n",
    "\n",
    "        grads = tape.gradient(loss, gnn_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, gnn_model.trainable_variables))\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PPO TRAINING EPISODE\n",
    "# ===================================================================\n",
    "def ppo_train_episode(max_steps=900):\n",
    "\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start([\"sumo\", \"-c\", CONFIG_FILE, \"--step-length\", \"1\"])\n",
    "\n",
    "    states, actions, rewards, values, log_probs = [], [], [], [], []\n",
    "\n",
    "    num_tls = len(tls_ids)\n",
    "    n_actions = 2   # your model uses 2 phases per TLS\n",
    "\n",
    "    total_return = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "\n",
    "        # ---------------- build state ----------------\n",
    "        vec = []\n",
    "        for tls in tls_ids:\n",
    "            s = []\n",
    "            for lane in tls_lane_map[tls]:\n",
    "                s.append(traci.lane.getLastStepVehicleNumber(lane))\n",
    "                s.append(traci.lane.getWaitingTime(lane))\n",
    "            s += [0] * (feature_size - len(s))\n",
    "            vec.append(s)\n",
    "\n",
    "        state_np = np.array(vec, dtype=np.float32)[None, :]\n",
    "        state_tf = tf.convert_to_tensor(state_np, dtype=tf.float32)\n",
    "\n",
    "        # ---------------- model prediction ----------------\n",
    "        logits_tf, value_tf = gnn_model((state_tf, adj_batch_tf), training=False)\n",
    "        logits = logits_tf[0].numpy()\n",
    "        value = value_tf.numpy()[0,0]\n",
    "\n",
    "        flat_logits = logits.reshape(-1)\n",
    "        probs = tf.nn.softmax(flat_logits).numpy()\n",
    "\n",
    "        # ---------------- sample ACTION ----------------\n",
    "        flat_action = np.random.choice(len(probs), p=probs)\n",
    "        action_vec = unflatten_action(flat_action, num_tls, n_actions)\n",
    "        log_prob = np.log(probs[flat_action] + 1e-10)\n",
    "\n",
    "        # ---------------- apply action ----------------\n",
    "        for i, tls in enumerate(tls_ids):\n",
    "            traci.trafficlight.setPhase(tls, int(action_vec[i]))\n",
    "\n",
    "        traci.simulationStep()\n",
    "\n",
    "        r = compute_reward(tls_lane_map)\n",
    "        total_return += r\n",
    "\n",
    "        states.append(state_np[0])\n",
    "        actions.append(flat_action)\n",
    "        rewards.append(r)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        if (t+1) % 300 == 0:\n",
    "            print(f\"[PPO] Step {t+1}/{max_steps}, reward={r:.3f}\")\n",
    "\n",
    "    traci.close()\n",
    "\n",
    "    # ---------------- prepare PPO buffers ----------------\n",
    "    advantages, returns = compute_advantages(rewards, values)\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    log_probs = np.array(log_probs, dtype=np.float32)\n",
    "\n",
    "    # ---------------- PPO UPDATE ----------------\n",
    "    ppo_update(states, actions, log_probs, returns, advantages)\n",
    "\n",
    "    print(\"PPO episode return:\", total_return)\n",
    "    return total_return\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# TRAIN PPO FOR N EPISODES\n",
    "# ===================================================================\n",
    "def train_ppo(num_episodes=50):\n",
    "    returns = []\n",
    "    for ep in range(num_episodes):\n",
    "        print(f\"\\n========== PPO TRAINING EPISODE {ep+1}/{num_episodes} ==========\")\n",
    "        ret = ppo_train_episode(max_steps=900)\n",
    "        returns.append(ret)\n",
    "        print(f\"Episode {ep+1} return = {ret:.3f}\")\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200c1f9a-482c-4be3-89b1-ca9a6452c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== PPO TRAINING EPISODE 1/50 ==========\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ppo(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 198\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[1;34m(num_episodes)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========== PPO TRAINING EPISODE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ==========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 198\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ppo_train_episode(max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m900\u001b[39m)\n\u001b[0;32m    199\u001b[0m     returns\u001b[38;5;241m.\u001b[39mappend(ret)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m return = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 121\u001b[0m, in \u001b[0;36mppo_train_episode\u001b[1;34m(max_steps)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m traci\u001b[38;5;241m.\u001b[39misLoaded():\n\u001b[0;32m    120\u001b[0m     traci\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 121\u001b[0m traci\u001b[38;5;241m.\u001b[39mstart([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msumo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_FILE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--step-length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    123\u001b[0m states, actions, rewards, values, log_probs \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[0;32m    125\u001b[0m num_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tls_ids)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CONFIG_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "train_ppo(num_episodes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d8210-cd4b-4d2e-84b8-e985610bb71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
